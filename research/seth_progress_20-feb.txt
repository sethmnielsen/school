# Simulated Vision-Based Boat Landing - Seth Nielsen

This project is focused on developing a vision-based approach to the problem of autonomously landing a multirotor UAV on an arbitary ship at sea. Previous projects have relied on the placement of a fiducial marker at the desired landing location to estimate the relative pose of the boat with respect to the UAV. Because such an approach limits the ability of the UAV to land on any sea vessel to only those that have been specially prepared with the marker, we chose to design a method that could allow a UAV to land on an arbitrary boat of a given type without any modification beyond the basic requirement of providing a flat, cleared area of sufficient size to accommodate the UAV.

A photo-realistic, 3D simulated world containing a boat on moving waves with a flying UAV was created for this project. The ship has a flat area that serves as a landing pad for the multirotor. The multirotor is equipped with a simulated sensor suite of a monocular camera, IMU, and GPS. A pre-trained deep convolutional neural network was used to perform semantic image segmentation on the camera’s video feed in order to distinguish the boat from the ocean, followed by distinguishing the landing pad from the other parts of the boat. The neural network has shown promising performance in the task of identifying the boat. 
The simulator developed for this project combines two tools: Holodeck (created by the BYU Perception, Control and Cognition Lab) as a camera simulator, and ROSflight (developed by the BYU MAGICC Lab) is used to simulate the other sensors and to propagate the dynamics between each rendered Holodeck frame, allowing control and streaming of sensor data at upwards of 1 kHz between rendered images. The state of the UAV within the Holodeck world is overwritten by ROSflight’s simulated dynamics each frame, until a collision occurs, in which case the UAV’s state within ROSflight is overwritten by the state computed by Holodeck. 



To accomplish this task, we have designed a new method consisting of three stages that will utilize a combination of vision-based techniques. The stages will be as follows:

  1. *Long-range relative estimation*: a convolutional neural network trained on simulated and real boat images will take frames from the UAV’s camera feed as input and output a state vector containing the relative position and attitude of the boat with respect to the UAV. The UAV will search for, find, and fly towards the boat.
  2. *Landing site selection*: the UAV will orbit the boat and, using the Recursive-RANSAC algorithm, identify groups of visual features that create a planar surface. The largest flat plane found will be chosen as the landing site.
  3. *Landing sequence*: a region of pixels will be selected from within the convex hull of the coplanar features of the landing site. These features will be transformed from their 3D coordinates in the world frame to be reconstructed into an image residing on a plane parallel to the image plane, then upsampled by interpolation to enlarge the image. This will produce an approximate representation of the landing site as it would appear to the camera when closer and parallel to the camera's image plane (assuming the camera is pointed directly downwards in the UAV's body frame), effectively creating a target pose for the UAV relative to the landing site. The UAV can then be controlled to reach the target pose via visual servoing, where the real image seen by the camera would roughly match the transformed image. This process will be iterated until landing is achieved.

This method will be tested in the ROSFlight-Holodeck simulation environment. If the performance of the algorithm in simulation is good, then hardware tests are planned to follow.
